{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertExperiment2_eng.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNi0HWMqr1wTq6FbtI7ulK+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaodamie/PAN-CLEF---Profilling-Hate-Spreaders-on-Twiter/blob/main/BertExperiment2_eng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS6Nza_fLg8L"
      },
      "source": [
        "pip install -q tensorflow-text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJEsM1IVL70i"
      },
      "source": [
        "pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdymly0zl2Hu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOS5eyJBMHy6"
      },
      "source": [
        "import pandas as pd\n",
        "import os \n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as mpl\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow import feature_column\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import shutil\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization \n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "name=r'/content/drive/MyDrive/Dataset/pan21-author-profiling-training-2021-03-14/en//'\n",
        "\n",
        "xml_text=[]\n",
        "list_filenames=[]\n",
        "for file in os.listdir(name):\n",
        "        filename = os.fsdecode(file)\n",
        "        \n",
        "        if filename.endswith('.xml'):\n",
        "#             print(filename)   \n",
        "            list_filenames.append(filename.split(sep='.')[0])\n",
        "            try:\n",
        "                tree = ET.parse(name+file) \n",
        "                root =tree.getroot() \n",
        "                xml_document=[]\n",
        "                for neighbor in root.iter('document'):\n",
        "                    xml_document.append(neighbor.text) #neighbor.text.lower()\n",
        "                     \n",
        "                    #print(xml_document)\n",
        "#                 print('\\n\\n')\n",
        "                xml_text.append(\" \".join(xml_document))\n",
        "            except Exception as e:\n",
        "                print(e, name+file)\n",
        "#                 display(e, folder+file)\n",
        "        \n",
        "        elif filename.endswith('txt'):\n",
        "            fileObject = open(name+file)\n",
        "            data = fileObject.read().splitlines()\n",
        "            dat=[]\n",
        "            for i in data:\n",
        "                dat.append(i.split(sep=':::'))\n",
        "                \n",
        "                \n",
        "# (list_filenames,xml_text,dat)\n",
        "# es_xml_text.append(xml_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdL0VIVHzYb7"
      },
      "source": [
        "# join the labels to the text\n",
        "df1=pd.DataFrame(zip(list_filenames,xml_text),columns=('ids','xml_text')).set_index('ids').join(pd.DataFrame(dat,columns=('ids','labels')).set_index(['ids']))\n",
        "# v=pd.DataFrame(dat)\n",
        "# v['xml'] =xml_text\n",
        "\n",
        "\n",
        "# in future make a k-fold cross validation with sklearn\n",
        "train, test = train_test_split(df1, test_size=0.2)\n",
        "val, test = train_test_split(test, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')\n",
        "\n",
        "train_ds=train['xml_text'].astype(str).to_numpy()\n",
        "val_ds=val['xml_text'].astype(str).to_numpy()\n",
        "\n",
        "whole_dataset=df1['xml_text'].astype(str).to_numpy()\n",
        "whole_labels=df1['labels'].astype(int).to_numpy()\n",
        "\n",
        "train_labels=train['labels'].astype(int).to_numpy()\n",
        "val_labels=val['labels'].astype(int).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxEJLPvt8-8X"
      },
      "source": [
        "df1['xml_text']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5oEyVQ8mk89"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_ds,train_labels))\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((val_ds,val_labels))\n",
        "\n",
        "whole_ds=tf.data.Dataset.from_tensor_slices((whole_dataset,whole_labels))\n",
        "\n",
        "train_ds = train_ds.batch(BATCH_SIZE)\n",
        "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
        "\n",
        "# PREFETCH\n",
        "\n",
        "train_ds = train_ds.shuffle(5).prefetch(buffer_size=AUTOTUNE)\n",
        "valid_ds = valid_ds.shuffle(5).prefetch(buffer_size=AUTOTUNE)\n",
        "# whole_ds=whole_ds.shuffle(5).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66wxwVRoMq8u"
      },
      "source": [
        "bert_model_name = 'albert_en_base' \n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZabFd2yZNENR"
      },
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "\n",
        "text_test = 'this is such an amazing movie!'\n",
        "# tt=' '.join(text_test)\n",
        "text_preprocessed = bert_preprocess_model([text_test])\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6U5X-NgiW1V"
      },
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjczK4ZFNuxw"
      },
      "source": [
        "# best model so far\n",
        "\n",
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdV4Em2clyAN"
      },
      "source": [
        "# def build_classifier_model_1():\n",
        "#   text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "#   preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "#   encoder_inputs = preprocessing_layer(text_input)\n",
        "#   encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "#   outputs = encoder(encoder_inputs)\n",
        "#   net = outputs['pooled_output']\n",
        "#   net = tf.keras.layers.Dropout(0.25)(net)\n",
        "#   net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)\n",
        "#   return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXYh4-hCL0lU"
      },
      "source": [
        "# def build_classifier_model_2():\n",
        "#   text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "#   preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "#   encoder_inputs = preprocessing_layer(text_input)\n",
        "#   encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "#   outputs = encoder(encoder_inputs)\n",
        "#   net = outputs['pooled_output']\n",
        "#   net = tf.keras.layers.Dropout(0.1)(net)\n",
        "#   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "#   return tf.keras.Model(text_input, net)\n",
        "\n",
        "# classifier_model_2= build_classifier_model_2()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-x9VcuhNzJN"
      },
      "source": [
        "classifier_model = build_classifier_model()\n",
        "# classifier_model_1= build_classifier_model_1()\n",
        "# bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "# print(tf.sigmoid(bert_raw_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAXLoiLi0gfr"
      },
      "source": [
        "tf.keras.utils.plot_model(classifier_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q76PoRsoQXDD"
      },
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXFyMxgWVDBY"
      },
      "source": [
        "\n",
        "\n",
        "epochs = 50\n",
        "steps_per_epoch =  tf.data.experimental.cardinality(train_ds).numpy()\n",
        "# tf.data.experimental.cardinality(np.array(train_ds)).numpy()\n",
        "# len(train)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 1e-6\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "# monitoring the val_loss is the best way as monitoring actual training los results in overfitting ..\n",
        "mcp_save = ModelCheckpoint('bert.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "early_stop=EarlyStopping(monitor='val_loss', patience=5, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmfxVGFDyCHx"
      },
      "source": [
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJJSSSI_Ogfe"
      },
      "source": [
        "# print(f'Training model with {tfhub_handle_encoder}')\n",
        "# history = classifier_model_2.fit(train_ds,\n",
        "#                                validation_data=valid_ds,\n",
        "#                                epochs=epochs,callbacks=[mcp_save, early_stop], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqtAorUIVGSB"
      },
      "source": [
        "# Keep!\n",
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(train_ds,\n",
        "                               validation_data=valid_ds,\n",
        "                               epochs=epochs,callbacks=[mcp_save, early_stop], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf4FXJqhXTUa"
      },
      "source": [
        "classifier_model.save('/content/drive/MyDrive/Dataset/PanResults/es')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvJEfavHkxK0"
      },
      "source": [
        "print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(train_ds,\n",
        "                               validation_data=valid_ds,\n",
        "                               epochs=epochs,callbacks=[mcp_save, early_stop], verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoFE0FL583G1"
      },
      "source": [
        "# output_model=tf.keras.models.load_model(\"bert.h5\", compile)\n",
        "\n",
        "# classifier_model.load_weights('bert.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KIyvbh_hTyr"
      },
      "source": [
        "train_loss, train_acc = classifier_model.evaluate(train_ds, verbose=1)\n",
        "validation_loss, validation_acc =classifier_model.evaluate(valid_ds, verbose=1)\n",
        "#whole_loss, whole_acc = classifier_model.evaluate(whole_ds, verbose=0)\n",
        "# test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=1)\n",
        "\n",
        "print('* Accuracy on training set: %0.2f%%' % (100 * train_acc))\n",
        "print('* Accuracy on validation set: %0.2f%%' % (100 * validation_acc))\n",
        "#print('* Accuracy on whole set: %0.2f%%' % (100 * whole_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ri0YRCenLcg"
      },
      "source": [
        "reloaded=tf.saved_model.load('/content/drive/MyDrive/Dataset/PanResults/en/savedmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CReUtPtkTeCg"
      },
      "source": [
        "results=tf.sigmoid(reloaded(tf.constant(test['xml_text'])))\n",
        "#test_results=round()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGhrd78HgJIT"
      },
      "source": [
        "test['results_1']=results.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDauKHowhOD4"
      },
      "source": [
        "round(results[0][0].numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGe95Jk0gp7D"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbkwSNWElqOR"
      },
      "source": [
        "#working with the actual test data\n",
        "name=r'/content/drive/MyDrive/Dataset/pan21-author-profiling-test-without-gold/en//'\n",
        "\n",
        "xml_text=[]\n",
        "list_filenames=[]\n",
        "for file in os.listdir(name):\n",
        "        filename = os.fsdecode(file)\n",
        "        \n",
        "        if filename.endswith('.xml'):\n",
        "#             print(filename)   \n",
        "            list_filenames.append(filename.split(sep='.')[0])\n",
        "            try:\n",
        "                tree = ET.parse(name+file) \n",
        "                root =tree.getroot() \n",
        "                xml_document=[]\n",
        "                for neighbor in root.iter('document'):\n",
        "                    xml_document.append(neighbor.text)\n",
        "                     \n",
        "                    #print(xml_document)\n",
        "#                 print('\\n\\n')\n",
        "                xml_text.append(\" \".join(xml_document))\n",
        "            except Exception as e:\n",
        "                print(e, name+file)\n",
        "#                 display(e, folder+file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06lHzk7Yl6mE"
      },
      "source": [
        "df2=pd.DataFrame(zip(list_filenames,xml_text),columns=('ids','xml_text')).set_index('ids')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh-BplH60LhK"
      },
      "source": [
        "df2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVsjnjpr0j5C"
      },
      "source": [
        "test_results=tf.sigmoid(reloaded(tf.constant(df2['xml_text'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI4Lboysw-a-"
      },
      "source": [
        "\n",
        "for i in range(len(df2['xml_text'])):\n",
        "  # print(f'{df1.index[i]}: {round(results[i][0].numpy())}')\n",
        "  root = ET.Element('author', id=df2.index[i],lang='en',type=f'{round(test_results[i][0].numpy())}')\n",
        "  s = ET.tostring(root)\n",
        "  print(str(s)) \n",
        "  tree = ET.ElementTree(root)\n",
        "  tree.write(\"/content/drive/MyDrive/Dataset/PanResults/Test_Results/en/%s.xml\" %df2.index[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}