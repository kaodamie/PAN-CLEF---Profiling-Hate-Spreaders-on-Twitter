{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PAN21_Exp_ Spanish.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcugn8cDMCS7/u72KZADem",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaodamie/PAN-CLEF---Profilling-Hate-Spreaders-on-Twiter/blob/main/PAN21_Exp__Spanish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnP_IHyntErc"
      },
      "source": [
        "pip install -q tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXpvfXe6tQDG"
      },
      "source": [
        "pip install -q tf-models-official"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-xnPbdTtTQq"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoLy_zystYYh"
      },
      "source": [
        "import pandas as pd\n",
        "import os \n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as mpl\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow import feature_column\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import shutil\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization \n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "name=r'/content/drive/MyDrive/Dataset/pan21-author-profiling-training-2021-03-14/es//'\n",
        "\n",
        "xml_text=[]\n",
        "list_filenames=[]\n",
        "for file in os.listdir(name):\n",
        "        filename = os.fsdecode(file)\n",
        "        \n",
        "        if filename.endswith('.xml'):\n",
        "#             print(filename)   \n",
        "            list_filenames.append(filename.split(sep='.')[0])\n",
        "            try:\n",
        "                tree = ET.parse(name+file) \n",
        "                root =tree.getroot() \n",
        "                xml_document=[]\n",
        "                for neighbor in root.iter('document'):\n",
        "                    xml_document.append(neighbor.text)\n",
        "                     \n",
        "                    #print(xml_document)\n",
        "#                 print('\\n\\n')\n",
        "                xml_text.append(\" \".join(xml_document))\n",
        "            except Exception as e:\n",
        "                print(e, name+file)\n",
        "#                 display(e, folder+file)\n",
        "        \n",
        "        elif filename.endswith('txt'):\n",
        "            fileObject = open(name+file)\n",
        "            data = fileObject.read().splitlines()\n",
        "            dat=[]\n",
        "            for i in data:\n",
        "                dat.append(i.split(sep=':::'))\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVqsmhEeteZ-"
      },
      "source": [
        "df1=pd.DataFrame(zip(list_filenames,xml_text),columns=('ids','xml_text')).set_index('ids').join(pd.DataFrame(dat,columns=('ids','labels')).set_index(['ids']))\n",
        "# v=pd.DataFrame(dat)\n",
        "# v['xml'] =xml_text\n",
        "\n",
        "\n",
        "# in future make a k-fold cross validation with sklearn\n",
        "train, test = train_test_split(df1, test_size=0.2)\n",
        "val, test = train_test_split(test, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')\n",
        "\n",
        "train_ds=train['xml_text'].astype(str).to_numpy()\n",
        "val_ds=val['xml_text'].astype(str).to_numpy()\n",
        "\n",
        "whole_dataset=df1['xml_text'].astype(str).to_numpy()\n",
        "whole_labels=df1['labels'].astype(int).to_numpy()\n",
        "\n",
        "train_labels=train['labels'].astype(int).to_numpy()\n",
        "val_labels=val['labels'].astype(int).to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnPbgXCwuCEN"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_ds,train_labels))\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((val_ds,val_labels))\n",
        "\n",
        "whole_ds=tf.data.Dataset.from_tensor_slices((whole_dataset,whole_labels))\n",
        "\n",
        "train_ds = train_ds.batch(BATCH_SIZE)\n",
        "valid_ds = valid_ds.batch(BATCH_SIZE)\n",
        "\n",
        "# PREFETCH\n",
        "\n",
        "train_ds = train_ds.shuffle(5).prefetch(buffer_size=AUTOTUNE)\n",
        "valid_ds = valid_ds.shuffle(5).prefetch(buffer_size=AUTOTUNE)\n",
        "whole_ds=whole_ds.shuffle(5).prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh6CV76ve_i_"
      },
      "source": [
        "#google sentence encoder classifier\n",
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\",name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input) \n",
        "  encoder = hub.KerasLayer(\"https://tfhub.dev/google/LaBSE/2\",name='encoder')\n",
        "\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgjmEo2JxVVe"
      },
      "source": [
        "classifier_model = build_classifier_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKb3KJD7wNlg"
      },
      "source": [
        "classifier_model.compile(optimizer='adam',\n",
        "                         loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                         metrics=tf.metrics.BinaryAccuracy())\n",
        "\n",
        "mcp_save = ModelCheckpoint('multi_sentence_encoder.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "early_stop=EarlyStopping(monitor='val_loss', patience=3, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bklKmqxcwhJ9"
      },
      "source": [
        "#print(f'Training model with {tfhub_handle_encoder}')\n",
        "history = classifier_model.fit(train_ds,\n",
        "                               validation_data=valid_ds,\n",
        "                               epochs=10,callbacks=[mcp_save, early_stop], verbose=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7HccyAwjE2d"
      },
      "source": [
        "# classifier_model.save('/content/drive/MyDrive/Dataset/PanResults/es')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rm_272enaIr"
      },
      "source": [
        "reloaded=tf.saved_model.load('/content/drive/MyDrive/Dataset/PanResults/es_savedmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Gidtuen0dg"
      },
      "source": [
        "reloaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agCRaD5Cjfya"
      },
      "source": [
        "train_loss, train_acc = classifier_model.evaluate(train_ds, verbose=1)\n",
        "validation_loss, validation_acc =classifier_model.evaluate(valid_ds, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g_YRAc1j_-C"
      },
      "source": [
        "print('* Accuracy on training set: %0.2f%%' % (100 * train_acc))\n",
        "print('* Accuracy on validation set: %0.2f%%' % (100 * validation_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtzKhAm838sY"
      },
      "source": [
        "\n",
        "results=tf.sigmoid(classifier_model(tf.constant(test['xml_text'])))\n",
        "\n",
        "\n",
        "results=tf.sigmoid(reloaded(tf.constant(test['xml_text'])))\n",
        "#test_results=round()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3brkvJ-4moM"
      },
      "source": [
        "test['results']=results.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK-a5FTw6fV5"
      },
      "source": [
        "#  a sample testset after shuffling\n",
        "test \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsxQJJ8p4pMW"
      },
      "source": [
        "# another sample test after shuffling\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75dqqC1YfNWZ"
      },
      "source": [
        "#working with the actual test data\n",
        "name=r'/content/drive/MyDrive/Dataset/pan21-author-profiling-test-without-gold/es//'\n",
        "\n",
        "xml_text=[]\n",
        "list_filenames=[]\n",
        "for file in os.listdir(name):\n",
        "        filename = os.fsdecode(file)\n",
        "        \n",
        "        if filename.endswith('.xml'):\n",
        "#             print(filename)   \n",
        "            list_filenames.append(filename.split(sep='.')[0])\n",
        "            try:\n",
        "                tree = ET.parse(name+file) \n",
        "                root =tree.getroot() \n",
        "                xml_document=[]\n",
        "                for neighbor in root.iter('document'):\n",
        "                    xml_document.append(neighbor.text)\n",
        "                     \n",
        "                    #print(xml_document)\n",
        "#                 print('\\n\\n')\n",
        "                xml_text.append(\" \".join(xml_document))\n",
        "            except Exception as e:\n",
        "                print(e, name+file)\n",
        "#                 display(e, folder+file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_AkIxAIfnLq"
      },
      "source": [
        "df2=pd.DataFrame(zip(list_filenames,xml_text),columns=('ids','xml_text')).set_index('ids')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2iz3z5lhdQB"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpbviI0Dh4oB"
      },
      "source": [
        "# if reusing saved model, \n",
        "test_results=tf.sigmoid(reloaded(tf.constant(df2['xml_text'])))\n",
        "\n",
        "# if not reusing saved model then comment out top\n",
        "test_results=tf.sigmoid(classifier_model(tf.constant(df2['xml_text'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GSq2uAJfwCq"
      },
      "source": [
        "for i in range(len(df2['xml_text'])):\n",
        "  print(f'{df2.index[i]}: {round(test_results[i][0].numpy())}')\n",
        "  root = ET.Element('author', id=df2.index[i],lang='es',type=f'{round(test_results[i][0].numpy())}')\n",
        "  s = ET.tostring(root)\n",
        "  print(str(s)) \n",
        "  tree = ET.ElementTree(root)\n",
        "  # change directory\n",
        "  tree.write(\"/content/drive/MyDrive/Dataset/PanResults/Test_Results/es/%s.xml\" %df2.index[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}